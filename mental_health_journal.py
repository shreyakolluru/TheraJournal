# -*- coding: utf-8 -*-
"""Mental Health Journal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1--NAiw4FEE7jZiGLKGLubqrl_2AxTi-7
"""

!pip install transformers gradio accelerate --quiet

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import gradio as gr
import torch

summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")

chat_model_name = "tiiuae/falcon-7b-instruct"  # Or use 3B if low on RAM

tokenizer = AutoTokenizer.from_pretrained(chat_model_name)

# Define the desired torch_dtype and device before using them
torch_dtype = torch.float16
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load the model using device_map="auto" which handles device placement
model = AutoModelForCausalLM.from_pretrained(
    chat_model_name,
    torch_dtype=torch_dtype,
    device_map="auto" if device == "cuda" else None
)
# Removed the .to(device) call as device_map="auto" handles device placement

def chat_with_model(prompt):
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
    output = model.generate(input_ids, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response[len(prompt):].strip()

def chat_with_model(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):].strip()

def generate_summary(entry):
    summary = summarizer(entry, max_length=60, min_length=20, do_sample=False)
    return summary[0]['summary_text']

def ai_therapist_response(entry):
    prompt = f"You are a kind and understanding therapist. The user said: '{entry}'. Respond compassionately and supportively."
    return chat_with_model(prompt)

def suggest_actions(entry):
    if not entry.strip():
        return "Please enter a valid journal entry."

    prompt = f"Read this journal entry: '{entry}'. Suggest a few healthy coping strategies or self-care tips to support mental well-being."

    try:
        return chat_with_model(prompt)
    except Exception as e:
        return f"‚ö†Ô∏è Error generating suggestions: {str(e)}"

with gr.Blocks() as demo:
    gr.Markdown("## üß† AI Mental Health Journal (Free Version)")
    gr.Markdown("Write your thoughts. Get a summary, supportive reply, and self-care tips.")

    entry = gr.Textbox(lines=10, label="üìù Your Journal Entry")
    btn = gr.Button("üß† Analyze Entry")

    summary_out = gr.Textbox(label="‚úçÔ∏è Summary")
    therapist_out = gr.Textbox(label="üí¨ Therapist's Response")
    suggestion_out = gr.Textbox(label="üåø Wellness Suggestions")

    # ‚úÖ These must be INSIDE the 'with gr.Blocks()' block
    btn.click(fn=generate_summary, inputs=entry, outputs=summary_out)
    btn.click(fn=ai_therapist_response, inputs=entry, outputs=therapist_out)
    btn.click(fn=suggest_actions, inputs=entry, outputs=suggestion_out)

# ‚úÖ This can stay outside
demo.launch()